{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bae53a09-c04c-4916-a146-00e0bb188954",
      "metadata": {
        "id": "bae53a09-c04c-4916-a146-00e0bb188954"
      },
      "source": [
        "# L4: Support Data Insight Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd08797e-0262-47f6-a5ba-0f933b06b06a",
      "metadata": {
        "id": "fd08797e-0262-47f6-a5ba-0f933b06b06a"
      },
      "source": [
        "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ⏳ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc6c1339-5278-4daf-8e97-04c280bde548",
      "metadata": {
        "id": "cc6c1339-5278-4daf-8e97-04c280bde548"
      },
      "source": [
        "## Initial Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install crewai crewai-tools poetry vllm"
      ],
      "metadata": {
        "id": "E1057VQzVOF4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "E1057VQzVOF4"
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "Li81aQao49HO"
      },
      "id": "Li81aQao49HO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve > ollama.log &"
      ],
      "metadata": {
        "id": "LuZBvwge45i6"
      },
      "id": "LuZBvwge45i6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull jyan1/paligemma-mix-224"
      ],
      "metadata": {
        "id": "J4zmbYyZ4-l9"
      },
      "id": "J4zmbYyZ4-l9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d1ce58-9d2e-4349-acbd-49227e37d31c",
      "metadata": {
        "height": 200,
        "id": "41d1ce58-9d2e-4349-acbd-49227e37d31c"
      },
      "outputs": [],
      "source": [
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load environment variables\n",
        "# from helper import load_env\n",
        "# load_env()\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "from crewai import Agent, Task, Crew, LLM\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n",
        "os.environ[\"GEMINI_API_KEY\"]  = \"AIzaSyCGN5PEjC19vmLoX0ALvyKrRvSsJ87kqNE\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login  # Verifique se está autenticado"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvTLrsj-n5o_",
        "outputId": "eea7f3c3-d338-4c58-d8f8-f3da1fa58b86"
      },
      "id": "BvTLrsj-n5o_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "The token `CAD_LLM` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `CAD_LLM`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install vLLM from pip:\n",
        "!pip install vllm"
      ],
      "metadata": {
        "id": "QsmkPh9eS2Bx"
      },
      "id": "QsmkPh9eS2Bx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and run the model:\n",
        "!vllm serve google/paligemma2-3b-pt-224 --dtype=half\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QOgLYhQTMcV",
        "outputId": "d83c8433-e471-4cbe-eeac-4cedcd0a7a6f"
      },
      "id": "3QOgLYhQTMcV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-07 19:12:33.317711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-07 19:12:33.337460: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-07 19:12:33.343474: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-07 19:12:33.358144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-07 19:12:34.552280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO 01-07 19:12:36 api_server.py:712] vLLM API server version 0.6.6.post1\n",
            "INFO 01-07 19:12:36 api_server.py:713] args: Namespace(subparser='serve', model_tag='google/paligemma2-3b-pt-224', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='google/paligemma2-3b-pt-224', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='half', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7d07d084cee0>)\n",
            "INFO 01-07 19:12:36 api_server.py:199] Started engine process with PID 7987\n",
            "WARNING 01-07 19:12:37 config.py:2276] Casting torch.bfloat16 to torch.float16.\n",
            "2025-01-07 19:12:45.089243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-07 19:12:45.121788: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-07 19:12:45.131758: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-07 19:12:46.993984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING 01-07 19:12:51 config.py:2276] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 01-07 19:12:55 config.py:510] This model supports multiple tasks: {'classify', 'score', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
            "INFO 01-07 19:13:04 config.py:510] This model supports multiple tasks: {'embed', 'score', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
            "INFO 01-07 19:13:04 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='google/paligemma2-3b-pt-224', speculative_config=None, tokenizer='google/paligemma2-3b-pt-224', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/paligemma2-3b-pt-224, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n",
            "INFO 01-07 19:13:08 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 01-07 19:13:08 selector.py:129] Using XFormers backend.\n",
            "INFO 01-07 19:13:09 importing.py:15] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
            "INFO 01-07 19:13:09 model_runner.py:1094] Starting to load model google/paligemma2-3b-pt-224...\n",
            "INFO 01-07 19:13:09 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 01-07 19:13:09 selector.py:129] Using XFormers backend.\n",
            "WARNING 01-07 19:13:10 xformers.py:387] XFormers does not support logits soft cap. Outputs may be slightly off.\n",
            "INFO 01-07 19:13:10 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
            "model-00002-of-00002.safetensors:   0% 0.00/1.07G [00:00<?, ?B/s]\n",
            "model-00002-of-00002.safetensors:   1% 10.5M/1.07G [00:00<00:26, 40.4MB/s]\n",
            "model-00002-of-00002.safetensors:   2% 21.0M/1.07G [00:00<00:25, 41.3MB/s]\n",
            "model-00002-of-00002.safetensors:   3% 31.5M/1.07G [00:00<00:24, 42.7MB/s]\n",
            "model-00002-of-00002.safetensors:   4% 41.9M/1.07G [00:00<00:24, 42.8MB/s]\n",
            "model-00002-of-00002.safetensors:   5% 52.4M/1.07G [00:01<00:23, 42.9MB/s]\n",
            "model-00002-of-00002.safetensors:   6% 62.9M/1.07G [00:01<00:23, 42.8MB/s]\n",
            "model-00002-of-00002.safetensors:   7% 73.4M/1.07G [00:01<00:23, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:   8% 83.9M/1.07G [00:01<00:23, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:   9% 94.4M/1.07G [00:02<00:23, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  10% 105M/1.07G [00:02<00:22, 42.8MB/s] \n",
            "model-00002-of-00002.safetensors:  11% 115M/1.07G [00:02<00:22, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:  12% 126M/1.07G [00:02<00:22, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  13% 136M/1.07G [00:03<00:22, 42.5MB/s]\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.99G [00:04<03:51, 20.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 168M/1.07G [00:05<00:40, 22.5MB/s]\n",
            "model-00002-of-00002.safetensors:  17% 178M/1.07G [00:05<00:35, 25.4MB/s]\n",
            "model-00002-of-00002.safetensors:  18% 189M/1.07G [00:05<00:31, 28.4MB/s]\n",
            "model-00002-of-00002.safetensors:  19% 199M/1.07G [00:06<00:27, 31.2MB/s]\n",
            "model-00002-of-00002.safetensors:  20% 210M/1.07G [00:06<00:25, 33.8MB/s]\n",
            "model-00002-of-00002.safetensors:  21% 220M/1.07G [00:06<00:23, 35.8MB/s]\n",
            "model-00002-of-00002.safetensors:  22% 231M/1.07G [00:06<00:22, 37.5MB/s]\n",
            "model-00002-of-00002.safetensors:  23% 241M/1.07G [00:07<00:21, 38.9MB/s]\n",
            "model-00001-of-00002.safetensors:   5% 241M/4.99G [00:07<01:59, 39.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 252M/1.07G [00:07<00:20, 39.7MB/s]\n",
            "model-00002-of-00002.safetensors:  24% 262M/1.07G [00:07<00:20, 40.4MB/s]\n",
            "model-00002-of-00002.safetensors:  25% 273M/1.07G [00:07<00:19, 40.8MB/s]\n",
            "model-00002-of-00002.safetensors:  27% 294M/1.07G [00:08<00:18, 41.8MB/s]\n",
            "model-00002-of-00002.safetensors:  28% 304M/1.07G [00:08<00:18, 42.0MB/s]\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.99G [00:08<01:52, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 315M/1.07G [00:08<00:17, 42.1MB/s]\n",
            "model-00002-of-00002.safetensors:  30% 325M/1.07G [00:09<00:17, 42.3MB/s]\n",
            "model-00002-of-00002.safetensors:  31% 336M/1.07G [00:09<00:17, 42.2MB/s]\n",
            "model-00002-of-00002.safetensors:  32% 346M/1.07G [00:09<00:17, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  33% 357M/1.07G [00:09<00:16, 42.2MB/s]\n",
            "model-00002-of-00002.safetensors:  34% 367M/1.07G [00:10<00:18, 39.0MB/s]\n",
            "model-00002-of-00002.safetensors:  36% 388M/1.07G [00:10<00:15, 43.9MB/s]\n",
            "model-00002-of-00002.safetensors:  37% 398M/1.07G [00:10<00:15, 43.7MB/s]\n",
            "model-00002-of-00002.safetensors:  38% 409M/1.07G [00:11<00:15, 43.4MB/s]\n",
            "model-00002-of-00002.safetensors:  39% 419M/1.07G [00:11<00:15, 43.2MB/s]\n",
            "model-00002-of-00002.safetensors:  40% 430M/1.07G [00:11<00:14, 43.1MB/s]\n",
            "model-00002-of-00002.safetensors:  41% 440M/1.07G [00:11<00:14, 43.0MB/s]\n",
            "model-00002-of-00002.safetensors:  42% 451M/1.07G [00:12<00:14, 43.1MB/s]\n",
            "model-00002-of-00002.safetensors:  43% 461M/1.07G [00:12<00:14, 43.0MB/s]\n",
            "model-00002-of-00002.safetensors:  44% 472M/1.07G [00:12<00:13, 42.9MB/s]\n",
            "model-00002-of-00002.safetensors:  45% 482M/1.07G [00:12<00:13, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:  46% 493M/1.07G [00:13<00:13, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:  47% 503M/1.07G [00:13<00:13, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  48% 514M/1.07G [00:13<00:13, 42.1MB/s]\n",
            "model-00002-of-00002.safetensors:  49% 524M/1.07G [00:13<00:12, 42.1MB/s]\n",
            "model-00002-of-00002.safetensors:  50% 535M/1.07G [00:14<00:12, 42.2MB/s]\n",
            "model-00002-of-00002.safetensors:  51% 545M/1.07G [00:14<00:12, 42.4MB/s]\n",
            "model-00002-of-00002.safetensors:  52% 556M/1.07G [00:14<00:12, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  53% 566M/1.07G [00:14<00:11, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  54% 577M/1.07G [00:14<00:11, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:  55% 587M/1.07G [00:15<00:11, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:  56% 598M/1.07G [00:15<00:11, 42.7MB/s]\n",
            "model-00002-of-00002.safetensors:  57% 608M/1.07G [00:15<00:10, 42.4MB/s]\n",
            "model-00002-of-00002.safetensors:  58% 619M/1.07G [00:15<00:10, 42.4MB/s]\n",
            "model-00002-of-00002.safetensors:  59% 629M/1.07G [00:16<00:10, 42.2MB/s]\n",
            "model-00002-of-00002.safetensors:  60% 640M/1.07G [00:16<00:10, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  61% 650M/1.07G [00:16<00:09, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  62% 661M/1.07G [00:16<00:09, 42.7MB/s]\n",
            "model-00002-of-00002.safetensors:  63% 671M/1.07G [00:17<00:09, 42.8MB/s]\n",
            "model-00002-of-00002.safetensors:  64% 682M/1.07G [00:17<00:09, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:  65% 692M/1.07G [00:17<00:08, 42.8MB/s]\n",
            "model-00002-of-00002.safetensors:  66% 703M/1.07G [00:17<00:08, 42.8MB/s]\n",
            "model-00002-of-00002.safetensors:  67% 713M/1.07G [00:18<00:08, 42.8MB/s]\n",
            "model-00002-of-00002.safetensors:  68% 724M/1.07G [00:18<00:08, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:  69% 734M/1.07G [00:18<00:07, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:  69% 744M/1.07G [00:18<00:07, 42.4MB/s]\n",
            "model-00002-of-00002.safetensors:  70% 755M/1.07G [00:19<00:07, 42.2MB/s]\n",
            "model-00002-of-00002.safetensors:  71% 765M/1.07G [00:19<00:07, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  72% 776M/1.07G [00:19<00:06, 42.3MB/s]\n",
            "model-00002-of-00002.safetensors:  73% 786M/1.07G [00:19<00:06, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  74% 797M/1.07G [00:20<00:06, 42.5MB/s]\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.99G [00:20<01:38, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 807M/1.07G [00:20<00:06, 41.0MB/s]\n",
            "model-00002-of-00002.safetensors:  76% 818M/1.07G [00:20<00:05, 42.9MB/s]\n",
            "model-00002-of-00002.safetensors:  77% 828M/1.07G [00:20<00:05, 42.9MB/s]\n",
            "model-00002-of-00002.safetensors:  78% 839M/1.07G [00:21<00:06, 37.9MB/s]\n",
            "model-00002-of-00002.safetensors:  79% 849M/1.07G [00:21<00:05, 39.2MB/s]\n",
            "model-00002-of-00002.safetensors:  80% 860M/1.07G [00:21<00:05, 37.3MB/s]\n",
            "model-00002-of-00002.safetensors:  81% 870M/1.07G [00:22<00:05, 38.0MB/s]\n",
            "model-00002-of-00002.safetensors:  82% 881M/1.07G [00:22<00:04, 39.3MB/s]\n",
            "model-00002-of-00002.safetensors:  83% 891M/1.07G [00:22<00:04, 40.1MB/s]\n",
            "model-00002-of-00002.safetensors:  84% 902M/1.07G [00:22<00:04, 40.7MB/s]\n",
            "model-00002-of-00002.safetensors:  85% 912M/1.07G [00:23<00:03, 41.3MB/s]\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.99G [00:23<01:34, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 933M/1.07G [00:23<00:03, 43.3MB/s]\n",
            "model-00002-of-00002.safetensors:  88% 944M/1.07G [00:23<00:02, 43.9MB/s]\n",
            "model-00002-of-00002.safetensors:  89% 954M/1.07G [00:24<00:02, 43.3MB/s]\n",
            "model-00002-of-00002.safetensors:  90% 965M/1.07G [00:24<00:02, 43.2MB/s]\n",
            "model-00002-of-00002.safetensors:  91% 975M/1.07G [00:24<00:02, 42.7MB/s]\n",
            "model-00002-of-00002.safetensors:  92% 986M/1.07G [00:24<00:02, 41.9MB/s]\n",
            "model-00002-of-00002.safetensors:  93% 996M/1.07G [00:25<00:01, 42.1MB/s]\n",
            "model-00002-of-00002.safetensors:  94% 1.01G/1.07G [00:25<00:01, 42.1MB/s]\n",
            "model-00002-of-00002.safetensors:  95% 1.02G/1.07G [00:25<00:01, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  96% 1.03G/1.07G [00:25<00:01, 42.6MB/s]\n",
            "model-00002-of-00002.safetensors:  97% 1.04G/1.07G [00:26<00:00, 42.7MB/s]\n",
            "model-00002-of-00002.safetensors:  98% 1.05G/1.07G [00:26<00:00, 42.5MB/s]\n",
            "model-00002-of-00002.safetensors:  99% 1.06G/1.07G [00:26<00:00, 42.4MB/s]\n",
            "model-00002-of-00002.safetensors: 100% 1.07G/1.07G [00:26<00:00, 42.4MB/s]\n",
            "model-00002-of-00002.safetensors: 100% 1.07G/1.07G [00:26<00:00, 39.9MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 1.10G/4.99G [00:27<01:31, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.99G [00:27<01:35, 40.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.12G/4.99G [00:27<01:34, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.99G [00:27<01:43, 37.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.99G [00:28<01:28, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.99G [00:28<01:28, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/4.99G [00:28<01:29, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.99G [00:28<01:28, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.99G [00:29<01:28, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/4.99G [00:29<01:28, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.99G [00:29<01:28, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/4.99G [00:29<01:28, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.99G [00:30<01:27, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.24G/4.99G [00:30<01:27, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.99G [00:30<01:27, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.99G [00:30<01:27, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.27G/4.99G [00:31<01:26, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.99G [00:31<01:26, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.29G/4.99G [00:31<01:26, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.99G [00:31<01:26, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.99G [00:32<01:27, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.32G/4.99G [00:32<01:34, 38.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.99G [00:32<01:32, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.99G [00:32<01:32, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.99G [00:33<01:30, 40.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.99G [00:33<01:28, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.99G [00:33<01:26, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.99G [00:33<01:25, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/4.99G [00:34<01:25, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.99G [00:34<01:24, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.42G/4.99G [00:34<01:24, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.99G [00:34<01:23, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.99G [00:35<01:23, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.45G/4.99G [00:35<01:23, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.99G [00:35<01:22, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.47G/4.99G [00:35<01:23, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.99G [00:36<01:22, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/4.99G [00:36<01:22, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.99G [00:36<01:22, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.51G/4.99G [00:36<01:21, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.52G/4.99G [00:37<01:20, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.99G [00:37<01:20, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.99G [00:37<01:20, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.99G [00:37<01:20, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.56G/4.99G [00:37<01:20, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.57G/4.99G [00:38<01:19, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.99G [00:38<01:19, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.99G [00:38<01:19, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.99G [00:38<01:18, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/4.99G [00:39<01:18, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.99G [00:39<01:18, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.99G [00:39<01:18, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.99G [00:39<01:18, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.66G/4.99G [00:40<01:17, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.67G/4.99G [00:40<01:17, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.99G [00:40<01:17, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.69G/4.99G [00:40<01:16, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.99G [00:41<01:16, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/4.99G [00:41<01:16, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.72G/4.99G [00:41<01:20, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.99G [00:41<01:14, 43.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.99G [00:42<01:15, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.99G [00:42<01:14, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.76G/4.99G [00:42<01:14, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.77G/4.99G [00:42<01:14, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.99G [00:43<01:16, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.99G [00:43<01:25, 37.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.99G [00:43<01:24, 37.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.99G [00:44<01:21, 39.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.99G [00:44<01:18, 40.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.99G [00:44<01:16, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.99G [00:44<01:15, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.99G [00:44<01:14, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.87G/4.99G [00:45<01:13, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.99G [00:45<01:13, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.99G [00:45<01:13, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.99G [00:45<01:12, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.91G/4.99G [00:46<01:12, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.92G/4.99G [00:46<01:11, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.99G [00:46<01:11, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.99G [00:46<01:10, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.99G [00:47<01:10, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.99G [00:47<01:10, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.97G/4.99G [00:47<01:10, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.99G [00:47<01:10, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.99G [00:48<01:10, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.99G [00:48<01:09, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.01G/4.99G [00:48<01:09, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.99G [00:48<01:09, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.99G [00:49<01:09, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/4.99G [00:49<01:08, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.99G [00:49<01:08, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.07G/4.99G [00:49<01:08, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.99G [00:50<01:07, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.99G [00:50<01:07, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.99G [00:50<01:07, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/4.99G [00:50<01:07, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.12G/4.99G [00:51<01:06, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.99G [00:51<01:04, 44.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.99G [00:51<01:04, 44.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.99G [00:51<01:04, 43.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.16G/4.99G [00:52<01:05, 43.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.17G/4.99G [00:52<01:05, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.99G [00:52<01:05, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.99G [00:52<01:05, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.20G/4.99G [00:53<01:05, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.99G [00:53<01:04, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.99G [00:53<01:04, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.99G [00:53<01:04, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.99G [00:54<01:11, 38.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.25G/4.99G [00:54<01:17, 35.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.26G/4.99G [00:54<01:19, 34.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.99G [00:55<01:14, 36.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/4.99G [00:55<01:10, 38.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.99G [00:55<01:08, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.99G [00:55<01:06, 40.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.32G/4.99G [00:55<01:05, 41.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.33G/4.99G [00:56<01:04, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.99G [00:56<01:03, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.99G [00:56<01:02, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/4.99G [00:56<01:02, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.37G/4.99G [00:57<01:01, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.99G [00:57<01:01, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.99G [00:57<01:01, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/4.99G [00:57<01:00, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.41G/4.99G [00:58<01:00, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.42G/4.99G [00:58<01:00, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.99G [00:58<00:59, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.99G [00:58<00:59, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.99G [00:59<00:59, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.46G/4.99G [00:59<00:59, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.99G [00:59<00:59, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.99G [00:59<00:58, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.99G [01:00<00:58, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.99G [01:00<00:58, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.52G/4.99G [01:00<00:57, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/4.99G [01:00<00:57, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.99G [01:01<00:57, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.55G/4.99G [01:01<00:57, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/4.99G [01:01<00:56, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.57G/4.99G [01:01<00:56, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.99G [01:02<00:56, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.99G [01:02<00:56, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.60G/4.99G [01:02<00:55, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.61G/4.99G [01:02<00:55, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.62G/4.99G [01:03<00:55, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/4.99G [01:03<00:54, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.99G [01:03<00:54, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.99G [01:03<01:03, 37.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.66G/4.99G [01:04<00:51, 44.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/4.99G [01:04<00:52, 44.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.99G [01:04<00:52, 43.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.99G [01:04<00:57, 40.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.99G [01:05<01:01, 37.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.72G/4.99G [01:05<01:01, 36.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.99G [01:05<01:00, 37.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/4.99G [01:06<00:57, 39.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.99G [01:06<00:55, 40.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.76G/4.99G [01:06<00:54, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.77G/4.99G [01:06<00:54, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/4.99G [01:07<00:53, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.99G [01:07<00:52, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.80G/4.99G [01:07<00:51, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.81G/4.99G [01:07<00:51, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.82G/4.99G [01:07<00:51, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/4.99G [01:08<00:50, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.99G [01:08<00:50, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/4.99G [01:08<00:49, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.86G/4.99G [01:08<00:49, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/4.99G [01:09<00:49, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.99G [01:09<00:49, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/4.99G [01:09<00:48, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.90G/4.99G [01:09<00:48, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.92G/4.99G [01:10<00:48, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.99G [01:10<00:48, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.99G [01:10<00:48, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.95G/4.99G [01:10<00:47, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.96G/4.99G [01:11<00:47, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.97G/4.99G [01:11<00:47, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.99G [01:11<00:47, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.99G [01:11<00:46, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.00G/4.99G [01:12<00:46, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.01G/4.99G [01:12<00:46, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.02G/4.99G [01:12<00:45, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.99G [01:12<00:45, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.99G [01:13<00:45, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.99G [01:13<00:45, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.06G/4.99G [01:13<00:44, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.99G [01:13<00:44, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.99G [01:14<00:44, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.09G/4.99G [01:14<00:44, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.10G/4.99G [01:14<00:44, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.11G/4.99G [01:14<00:43, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.12G/4.99G [01:15<00:43, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.99G [01:15<00:48, 38.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.99G [01:15<00:41, 44.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/4.99G [01:15<00:50, 36.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.17G/4.99G [01:16<00:47, 38.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.99G [01:16<00:46, 39.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.19G/4.99G [01:16<00:46, 38.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/4.99G [01:16<00:45, 39.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.21G/4.99G [01:17<00:43, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.22G/4.99G [01:17<00:43, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.99G [01:17<00:42, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.99G [01:17<00:41, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.25G/4.99G [01:18<00:41, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.26G/4.99G [01:18<00:40, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.99G [01:18<00:40, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.99G [01:18<00:40, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/4.99G [01:19<00:39, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.99G [01:19<00:39, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.31G/4.99G [01:19<00:39, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.99G [01:19<00:39, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/4.99G [01:20<00:38, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/4.99G [01:20<00:38, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.36G/4.99G [01:20<00:38, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.37G/4.99G [01:20<00:37, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.38G/4.99G [01:21<00:37, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.99G [01:21<00:38, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.40G/4.99G [01:21<00:37, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.41G/4.99G [01:21<00:37, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.42G/4.99G [01:22<00:37, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.99G [01:22<00:36, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.44G/4.99G [01:22<00:36, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.99G [01:22<00:36, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.46G/4.99G [01:23<00:36, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.99G [01:23<00:35, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.99G [01:23<00:35, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.99G [01:23<00:34, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.50G/4.99G [01:24<00:34, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.51G/4.99G [01:24<00:34, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.99G [01:24<00:34, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/4.99G [01:24<00:34, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.99G [01:25<00:33, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/4.99G [01:25<00:33, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.57G/4.99G [01:25<00:41, 34.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.99G [01:26<00:31, 45.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/4.99G [01:26<00:31, 44.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.61G/4.99G [01:26<00:31, 44.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.62G/4.99G [01:26<00:37, 37.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.99G [01:27<00:40, 33.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.64G/4.99G [01:27<00:38, 34.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.99G [01:27<00:36, 36.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.66G/4.99G [01:28<00:34, 38.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.67G/4.99G [01:28<00:33, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.99G [01:28<00:32, 40.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.99G [01:28<00:31, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.70G/4.99G [01:29<00:31, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.71G/4.99G [01:29<00:30, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.72G/4.99G [01:29<00:30, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.99G [01:29<00:29, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.99G [01:30<00:29, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/4.99G [01:30<00:29, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.76G/4.99G [01:30<00:28, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.99G [01:30<00:28, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.79G/4.99G [01:31<00:28, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/4.99G [01:31<00:28, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.81G/4.99G [01:31<00:27, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.82G/4.99G [01:31<00:27, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.99G [01:32<00:27, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.99G [01:32<00:26, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.85G/4.99G [01:32<00:26, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.86G/4.99G [01:32<00:26, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.87G/4.99G [01:33<00:26, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/4.99G [01:33<00:26, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/4.99G [01:33<00:25, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.99G [01:33<00:25, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.91G/4.99G [01:33<00:25, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.99G [01:34<00:25, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.99G [01:34<00:24, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/4.99G [01:34<00:24, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.95G/4.99G [01:34<00:24, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.96G/4.99G [01:35<00:24, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.99G [01:35<00:23, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/4.99G [01:35<00:23, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.99G [01:35<00:23, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.01G/4.99G [01:36<00:23, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.02G/4.99G [01:36<00:22, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.99G [01:36<00:22, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.99G [01:36<00:22, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.05G/4.99G [01:37<00:22, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.06G/4.99G [01:37<00:21, 43.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.07G/4.99G [01:37<00:26, 35.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.99G [01:38<00:24, 37.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.09G/4.99G [01:38<00:24, 36.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.99G [01:38<00:23, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.11G/4.99G [01:38<00:22, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.99G [01:39<00:21, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/4.99G [01:39<00:20, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.99G [01:39<00:20, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.15G/4.99G [01:39<00:20, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.16G/4.99G [01:40<00:19, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/4.99G [01:40<00:19, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.99G [01:40<00:19, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/4.99G [01:40<00:18, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.20G/4.99G [01:41<00:18, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.22G/4.99G [01:41<00:18, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.99G [01:41<00:17, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.24G/4.99G [01:41<00:17, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.99G [01:42<00:17, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.26G/4.99G [01:42<00:17, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.27G/4.99G [01:42<00:17, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/4.99G [01:42<00:16, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.99G [01:43<00:16, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.30G/4.99G [01:43<00:16, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.31G/4.99G [01:43<00:15, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.99G [01:43<00:15, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.99G [01:44<00:15, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.34G/4.99G [01:44<00:18, 36.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.36G/4.99G [01:44<00:13, 45.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/4.99G [01:44<00:13, 44.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.99G [01:45<00:13, 44.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.99G [01:45<00:13, 43.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.40G/4.99G [01:45<00:13, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.41G/4.99G [01:45<00:13, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.99G [01:46<00:13, 43.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.99G [01:46<00:12, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.99G [01:46<00:12, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.46G/4.99G [01:46<00:12, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.47G/4.99G [01:47<00:12, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/4.99G [01:47<00:12, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.49G/4.99G [01:47<00:11, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.99G [01:47<00:11, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.51G/4.99G [01:48<00:10, 45.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.53G/4.99G [01:48<00:06, 68.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/4.99G [01:48<00:06, 66.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.55G/4.99G [01:48<00:08, 55.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.56G/4.99G [01:49<00:09, 44.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/4.99G [01:49<00:09, 43.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.99G [01:49<00:09, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.99G [01:49<00:09, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.60G/4.99G [01:50<00:09, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.61G/4.99G [01:50<00:08, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/4.99G [01:50<00:08, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.63G/4.99G [01:50<00:08, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.99G [01:51<00:08, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.66G/4.99G [01:51<00:07, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.67G/4.99G [01:51<00:07, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/4.99G [01:51<00:07, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/4.99G [01:52<00:07, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.70G/4.99G [01:52<00:06, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.71G/4.99G [01:52<00:06, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.72G/4.99G [01:52<00:06, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/4.99G [01:52<00:06, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/4.99G [01:53<00:05, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.75G/4.99G [01:53<00:05, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.76G/4.99G [01:53<00:05, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.99G [01:53<00:05, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.78G/4.99G [01:54<00:04, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/4.99G [01:54<00:04, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.80G/4.99G [01:54<00:04, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.81G/4.99G [01:54<00:04, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.82G/4.99G [01:55<00:04, 34.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.84G/4.99G [01:55<00:03, 45.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.85G/4.99G [01:55<00:03, 44.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.87G/4.99G [01:56<00:02, 44.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/4.99G [01:56<00:02, 43.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.89G/4.99G [01:56<00:02, 43.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/4.99G [01:56<00:02, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.91G/4.99G [01:57<00:01, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.92G/4.99G [01:57<00:01, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.93G/4.99G [01:57<00:01, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.94G/4.99G [01:57<00:01, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.95G/4.99G [01:58<00:01, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.96G/4.99G [01:58<00:00, 48.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.98G/4.99G [01:58<00:00, 73.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.99G/4.99G [01:58<00:00, 42.1MB/s]\n",
            "model.safetensors.index.json: 100% 75.1k/75.1k [00:00<00:00, 1.18MB/s]\n",
            "Loading safetensors checkpoint shards: 100% 2/2 [00:24<00:00, 12.31s/it]\n",
            "INFO 01-07 19:15:34 model_runner.py:1099] Loading model weights took 5.6885 GB\n",
            "INFO 01-07 19:15:39 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250107-191539.pkl...\n",
            "INFO 01-07 19:15:39 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20250107-191539.pkl.\n",
            "ERROR 01-07 19:15:39 engine.py:366] BackendCompilerFailed.__init__() missing 1 required positional argument: 'inner_exception'\n",
            "ERROR 01-07 19:15:39 engine.py:366] Traceback (most recent call last):\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return aot_autograd(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_fn = dispatch_and_compile()\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return _create_aot_dispatcher_function(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_fn, fw_metadata = compiler_fn(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return inner_compile(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "ERROR 01-07 19:15:39 engine.py:366]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_graph = FxGraphCache.load(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_graph = compile_fx_fn(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_fn = graph.compile_to_fn()\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self.compile_to_module().call\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self._compile_to_module()\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.scheduler = Scheduler(self.operations)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self._init(nodes)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return SchedulerNode(self, node)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self._compute_attrs()\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "ERROR 01-07 19:15:39 engine.py:366]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.backends[device] = self.create_backend(device)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "ERROR 01-07 19:15:39 engine.py:366]     raise RuntimeError(\n",
            "ERROR 01-07 19:15:39 engine.py:366] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "ERROR 01-07 19:15:39 engine.py:366] \n",
            "ERROR 01-07 19:15:39 engine.py:366] The above exception was the direct cause of the following exception:\n",
            "ERROR 01-07 19:15:39 engine.py:366] \n",
            "ERROR 01-07 19:15:39 engine.py:366] Traceback (most recent call last):\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return func(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1691, in execute_model\n",
            "ERROR 01-07 19:15:39 engine.py:366]     hidden_or_intermediate_states = model_executable(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self._call_impl(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return forward_call(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/paligemma.py\", line 294, in forward\n",
            "ERROR 01-07 19:15:39 engine.py:366]     hidden_states = self.language_model.model(input_ids,\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py\", line 168, in __call__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self.forward(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/gemma2.py\", line 304, in forward\n",
            "ERROR 01-07 19:15:39 engine.py:366]     hidden_states, residual = layer(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self._call_impl(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return forward_call(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/gemma2.py\", line 229, in forward\n",
            "ERROR 01-07 19:15:39 engine.py:366]     hidden_states = self.input_layernorm(hidden_states)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self._call_impl(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return forward_call(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/custom_op.py\", line 24, in forward\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self._forward_method(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/layernorm.py\", line 212, in forward_cuda\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self.forward_native(x, residual)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/layernorm.py\", line 197, in forward_native\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self.forward_static(self.weight.data, self.variance_epsilon, x,\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return fn(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1269, in __call__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self._torchdynamo_orig_callable(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     result = self._inner_convert(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return _compile(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "ERROR 01-07 19:15:39 engine.py:366]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return function(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "ERROR 01-07 19:15:39 engine.py:366]     out_code = transform_code_object(code, transform)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "ERROR 01-07 19:15:39 engine.py:366]     transformations(instructions, code_options)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return fn(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "ERROR 01-07 19:15:39 engine.py:366]     tracer.run()\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "ERROR 01-07 19:15:39 engine.py:366]     super().run()\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "ERROR 01-07 19:15:39 engine.py:366]     while self.step():\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.dispatch_table[inst.opcode](self, inst)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self._return(inst)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.output.compile_subgraph(\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "ERROR 01-07 19:15:39 engine.py:366]     compiled_fn = self.call_user_compiler(gm)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self._call_user_compiler(gm)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "ERROR 01-07 19:15:39 engine.py:366]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "ERROR 01-07 19:15:39 engine.py:366] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "ERROR 01-07 19:15:39 engine.py:366] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "ERROR 01-07 19:15:39 engine.py:366] \n",
            "ERROR 01-07 19:15:39 engine.py:366] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "ERROR 01-07 19:15:39 engine.py:366] \n",
            "ERROR 01-07 19:15:39 engine.py:366] \n",
            "ERROR 01-07 19:15:39 engine.py:366] You can suppress this exception and fall back to eager by setting:\n",
            "ERROR 01-07 19:15:39 engine.py:366]     import torch._dynamo\n",
            "ERROR 01-07 19:15:39 engine.py:366]     torch._dynamo.config.suppress_errors = True\n",
            "ERROR 01-07 19:15:39 engine.py:366] \n",
            "ERROR 01-07 19:15:39 engine.py:366] \n",
            "ERROR 01-07 19:15:39 engine.py:366] During handling of the above exception, another exception occurred:\n",
            "ERROR 01-07 19:15:39 engine.py:366] \n",
            "ERROR 01-07 19:15:39 engine.py:366] Traceback (most recent call last):\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\n",
            "ERROR 01-07 19:15:39 engine.py:366]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return cls(ipc_path=ipc_path,\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.engine = LLMEngine(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 276, in __init__\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self._initialize_kv_caches()\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 416, in _initialize_kv_caches\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.model_executor.determine_num_available_blocks())\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 68, in determine_num_available_blocks\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return self.driver_worker.determine_num_available_blocks()\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return func(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 202, in determine_num_available_blocks\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.model_runner.profile_run()\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return func(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1331, in profile_run\n",
            "ERROR 01-07 19:15:39 engine.py:366]     self.execute_model(model_input, kv_caches, intermediate_tensors)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "ERROR 01-07 19:15:39 engine.py:366]     return func(*args, **kwargs)\n",
            "ERROR 01-07 19:15:39 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\n",
            "ERROR 01-07 19:15:39 engine.py:366]     raise type(err)(\n",
            "ERROR 01-07 19:15:39 engine.py:366] TypeError: BackendCompilerFailed.__init__() missing 1 required positional argument: 'inner_exception'\n",
            "Process SpawnProcess-1:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "    compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "    compiled_gm = compiler_fn(gm, example_inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "    return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "    return aot_autograd(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "    compiled_fn = dispatch_and_compile()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "    compiled_fn, _ = create_aot_dispatcher_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "    return _create_aot_dispatcher_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "    compiled_fn, fw_metadata = compiler_fn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "    compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "    return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "    return inner_compile(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "    return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "    inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "    compiled_graph = FxGraphCache.load(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "    compiled_graph = compile_fx_fn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "    compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "    compiled_fn = graph.compile_to_fn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "    return self.compile_to_module().call\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "    return self._compile_to_module()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "    self.scheduler = Scheduler(self.operations)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "    self._init(nodes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "    self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "    self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "    return SchedulerNode(self, node)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "    self._compute_attrs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "    group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "    self.backends[device] = self.create_backend(device)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1691, in execute_model\n",
            "    hidden_or_intermediate_states = model_executable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/paligemma.py\", line 294, in forward\n",
            "    hidden_states = self.language_model.model(input_ids,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py\", line 168, in __call__\n",
            "    return self.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/gemma2.py\", line 304, in forward\n",
            "    hidden_states, residual = layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/gemma2.py\", line 229, in forward\n",
            "    hidden_states = self.input_layernorm(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/custom_op.py\", line 24, in forward\n",
            "    return self._forward_method(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/layernorm.py\", line 212, in forward_cuda\n",
            "    return self.forward_native(x, residual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/layernorm.py\", line 197, in forward_native\n",
            "    return self.forward_static(self.weight.data, self.variance_epsilon, x,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1269, in __call__\n",
            "    return self._torchdynamo_orig_callable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "    result = self._inner_convert(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "    return _compile(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "    return _compile_inner(code, one_graph, hooks, transform)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "    return function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "    out_code = transform_code_object(code, transform)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "    transformations(instructions, code_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "    tracer.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "    super().run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "    while self.step():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "    self.dispatch_table[inst.opcode](self, inst)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "    self._return(inst)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "    self.output.compile_subgraph(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "    compiled_fn = self.call_user_compiler(gm)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "    return self._call_user_compiler(gm)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "\n",
            "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "\n",
            "\n",
            "You can suppress this exception and fall back to eager by setting:\n",
            "    import torch._dynamo\n",
            "    torch._dynamo.config.suppress_errors = True\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 368, in run_mp_engine\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\n",
            "    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\n",
            "    return cls(ipc_path=ipc_path,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\n",
            "    self.engine = LLMEngine(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 276, in __init__\n",
            "    self._initialize_kv_caches()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 416, in _initialize_kv_caches\n",
            "    self.model_executor.determine_num_available_blocks())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 68, in determine_num_available_blocks\n",
            "    return self.driver_worker.determine_num_available_blocks()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 202, in determine_num_available_blocks\n",
            "    self.model_runner.profile_run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1331, in profile_run\n",
            "    self.execute_model(model_input, kv_caches, intermediate_tensors)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\n",
            "    raise type(err)(\n",
            "TypeError: BackendCompilerFailed.__init__() missing 1 required positional argument: 'inner_exception'\n",
            "[rank0]:[W107 19:15:40.825836329 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-2' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-3' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-4' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-5' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-6' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-7' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-8' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-9' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-10' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-11' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-12' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-13' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-14' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-15' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-16' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-17' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\n",
            "    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 383, in poll\n",
            "    raise _zmq.ZMQError(_zmq.ENOTSUP)\n",
            "zmq.error.ZMQError: Operation not supported\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/vllm\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/scripts.py\", line 201, in main\n",
            "    args.dispatch_function(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/scripts.py\", line 42, in serve\n",
            "    uvloop.run(run_server(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvloop/__init__.py\", line 82, in run\n",
            "    return loop.run_until_complete(wrapper())\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n",
            "    return await main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 740, in run_server\n",
            "    async with build_async_engine_client(args) as engine_client:\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
            "    return await anext(self.gen)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 118, in build_async_engine_client\n",
            "    async with build_async_engine_client_from_engine_args(\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
            "    return await anext(self.gen)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 223, in build_async_engine_client_from_engine_args\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Engine process failed to start. See stack trace for the root cause.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(\n",
        "    model=\"google/paligemma\",\n",
        "    api_key=os.environ[\"GEMINI_API_KEY\"]\n",
        "    # ,\n",
        "    # auth_token=os.environ[\"HUGGINGFACE_TOKEN\"]\n",
        ")"
      ],
      "metadata": {
        "id": "meV7cy_QLPoH"
      },
      "id": "meV7cy_QLPoH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(llm.generate('test message'))\n",
        "# print(dir(llm))"
      ],
      "metadata": {
        "id": "2ghSnLazkK3C"
      },
      "id": "2ghSnLazkK3C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fe1b574a-502a-49ec-9dc4-1e4e32754986",
      "metadata": {
        "id": "fe1b574a-502a-49ec-9dc4-1e4e32754986"
      },
      "source": [
        "## Loading Tasks and Agents YAML files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f171b23-b3cc-48c9-a8ba-07358e271e63",
      "metadata": {
        "height": 268,
        "id": "0f171b23-b3cc-48c9-a8ba-07358e271e63"
      },
      "outputs": [],
      "source": [
        "# Define file paths for YAML configurations\n",
        "files = {\n",
        "    'agents': 'agents.yaml',\n",
        "    'tasks': 'tasks.yaml'\n",
        "}\n",
        "\n",
        "# Load configurations from YAML files\n",
        "configs = {}\n",
        "for config_type, file_path in files.items():\n",
        "    with open(file_path, 'r') as file:\n",
        "        configs[config_type] = yaml.safe_load(file)\n",
        "\n",
        "# Assign loaded configurations to specific variables\n",
        "agents_config = configs['agents']\n",
        "tasks_config = configs['tasks']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27064ec0-4a12-4dbd-b9b2-c77d4aaaedb5",
      "metadata": {
        "id": "27064ec0-4a12-4dbd-b9b2-c77d4aaaedb5"
      },
      "source": [
        "## Using FileReadTool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f58e827a-8e2e-4f1d-8f8c-9486828f951d",
      "metadata": {
        "height": 47,
        "id": "f58e827a-8e2e-4f1d-8f8c-9486828f951d"
      },
      "outputs": [],
      "source": [
        "from crewai_tools import VisionTool\n",
        "vision_tool = VisionTool()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J3xhKQ1Lrv6R"
      },
      "id": "J3xhKQ1Lrv6R"
    },
    {
      "cell_type": "markdown",
      "id": "7dc509e7-5aae-4795-a5f1-822cd5bfaefa",
      "metadata": {
        "id": "7dc509e7-5aae-4795-a5f1-822cd5bfaefa"
      },
      "source": [
        "## Creating Agents, Tasks and Crew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d2c719-9823-4d12-bd87-219f5145f7dc",
      "metadata": {
        "height": 948,
        "id": "33d2c719-9823-4d12-bd87-219f5145f7dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90fe115-be66-4704-ac11-b5d983c783eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating Agents\n",
        "diagnostic_agent = Agent(\n",
        "  config=agents_config['diagnostic_agent'],\n",
        "  tools=[vision_tool],\n",
        "  llm=llm\n",
        ")\n",
        "\n",
        "prognostic_agent = Agent(\n",
        "  config=agents_config['prognostic_agent'],\n",
        "  tools=[vision_tool],\n",
        "  llm=llm\n",
        ")\n",
        "\n",
        "imaging_analysis_agent = Agent(\n",
        "  config=agents_config['imaging_analysis_agent'],\n",
        "  tools=[vision_tool],\n",
        "  llm=llm\n",
        ")\n",
        "\n",
        "summarization_agent = Agent(\n",
        "  config=agents_config['summarization_agent'],\n",
        "  tools=[vision_tool],\n",
        "  llm=llm\n",
        ")\n",
        "\n",
        "# Creating Tasks\n",
        "melanoma_diagnosis_da = Task(\n",
        "  config=tasks_config['melanoma_diagnosis'],\n",
        "  agent=diagnostic_agent\n",
        ")\n",
        "melanoma_diagnosis_pa = Task(\n",
        "  config=tasks_config['melanoma_diagnosis'],\n",
        "  agent=prognostic_agent\n",
        ")\n",
        "melanoma_diagnosis_iaa = Task(\n",
        "  config=tasks_config['melanoma_diagnosis'],\n",
        "  agent=imaging_analysis_agent\n",
        ")\n",
        "\n",
        "combine_results_task = Task(\n",
        "    config=tasks_config['medical_diagnosis_summary'],\n",
        "    agent=summarization_agent,\n",
        "    context=[melanoma_diagnosis_da, melanoma_diagnosis_pa, melanoma_diagnosis_iaa]\n",
        ")\n",
        "\n",
        "# Creating Crew\n",
        "support_report_crew = Crew(\n",
        "  agents=[\n",
        "      diagnostic_agent,\n",
        "      prognostic_agent,\n",
        "      imaging_analysis_agent,\n",
        "      summarization_agent\n",
        "  ],\n",
        "  tasks=[\n",
        "    melanoma_diagnosis_da,\n",
        "    melanoma_diagnosis_pa,\n",
        "    melanoma_diagnosis_iaa,\n",
        "    combine_results_task\n",
        "  ],\n",
        "  verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299bca36-395b-45f2-92e7-27b403e32a27",
      "metadata": {
        "id": "299bca36-395b-45f2-92e7-27b403e32a27"
      },
      "source": [
        "## Testing our Crew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6646acc-e280-44cc-8968-08db37115eba",
      "metadata": {
        "height": 30,
        "id": "b6646acc-e280-44cc-8968-08db37115eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "370baed2-21fb-4ea8-c10a-ec1f086b9cd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'model_name' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=google/paligemma\n",
            " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=google/paligemma\n",
            " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
            "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
            "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=google/paligemma\n",
            " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mDermatologist\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze a skin lesion {image} using the 7-point checklist methodology for melanoma diagnosis.  This diagnostic framework evaluates specific dermatoscopic criteria categorized as major and minor.  The task involves: - Identifying major criteria: atypical pigment network, blue-white veil, and atypical vascular pattern. - Identifying minor criteria: irregular streaks, irregular pigmentation, irregular dots and globules, and regression structures. - Scoring each identified feature according to the checklist (3 points for major criteria, 1 point for minor criteria). - Summarizing the total score and providing a diagnostic interpretation based on established thresholds.\n",
            "The goal is to deliver a structured and evidence-based diagnosis of the lesion, helping clinicians assess malignancy risk accurately.\n",
            "\u001b[00m\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mDermatologist\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze a skin lesion {image} using the 7-point checklist methodology for melanoma diagnosis.  This diagnostic framework evaluates specific dermatoscopic criteria categorized as major and minor.  The task involves: - Identifying major criteria: atypical pigment network, blue-white veil, and atypical vascular pattern. - Identifying minor criteria: irregular streaks, irregular pigmentation, irregular dots and globules, and regression structures. - Scoring each identified feature according to the checklist (3 points for major criteria, 1 point for minor criteria). - Summarizing the total score and providing a diagnostic interpretation based on established thresholds.\n",
            "The goal is to deliver a structured and evidence-based diagnosis of the lesion, helping clinicians assess malignancy risk accurately.\n",
            "\u001b[00m\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mDermatologist\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze a skin lesion {image} using the 7-point checklist methodology for melanoma diagnosis.  This diagnostic framework evaluates specific dermatoscopic criteria categorized as major and minor.  The task involves: - Identifying major criteria: atypical pigment network, blue-white veil, and atypical vascular pattern. - Identifying minor criteria: irregular streaks, irregular pigmentation, irregular dots and globules, and regression structures. - Scoring each identified feature according to the checklist (3 points for major criteria, 1 point for minor criteria). - Summarizing the total score and providing a diagnostic interpretation based on established thresholds.\n",
            "The goal is to deliver a structured and evidence-based diagnosis of the lesion, helping clinicians assess malignancy risk accurately.\n",
            "\u001b[00m\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=google/paligemma\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             result = self.agent_executor.invoke(\n\u001b[0m\u001b[1;32m    334\u001b[0m                 {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask_for_human_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ask_for_human_input\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mformatted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_within_rpm_limit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_within_rpm_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     answer = self.llm.call(\n\u001b[0m\u001b[1;32m    116\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/llm.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, messages, callbacks)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m                 )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n\u001b[0;32m--> 998\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2957\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   2959\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"azure\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n\u001b[0m\u001b[1;32m    932\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\u001b[0m in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadRequestError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\u001b[0m in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             raise litellm.exceptions.BadRequestError(  # type: ignore\n\u001b[0m\u001b[1;32m    329\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=google/paligemma\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             result = self.agent_executor.invoke(\n\u001b[0m\u001b[1;32m    334\u001b[0m                 {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask_for_human_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ask_for_human_input\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mformatted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_within_rpm_limit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_within_rpm_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     answer = self.llm.call(\n\u001b[0m\u001b[1;32m    116\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/llm.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, messages, callbacks)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m                 )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n\u001b[0;32m--> 998\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2957\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   2959\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"azure\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n\u001b[0m\u001b[1;32m    932\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\u001b[0m in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadRequestError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\u001b[0m in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             raise litellm.exceptions.BadRequestError(  # type: ignore\n\u001b[0m\u001b[1;32m    329\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=google/paligemma\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-dd2c80acf113>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msupport_report_crew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/crew.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, n_iterations, openai_model_name, inputs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0mtest_crew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkickoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_crew_evaluation_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/crew.py\u001b[0m in \u001b[0;36mkickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sequential_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhierarchical\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_hierarchical_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/crew.py\u001b[0m in \u001b[0;36m_run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_sequential_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCrewOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;34m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_hierarchical_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCrewOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/crew.py\u001b[0m in \u001b[0;36m_execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                 task_output = task.execute_sync(\n\u001b[0m\u001b[1;32m    757\u001b[0m                     \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent_to_use\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                     \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/task.py\u001b[0m in \u001b[0;36mexecute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    300\u001b[0m     ) -> TaskOutput:\n\u001b[1;32m    301\u001b[0m         \u001b[0;34m\"\"\"Execute the task synchronously.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/task.py\u001b[0m in \u001b[0;36m_execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_by_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         result = agent.execute_task(\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_times_executed\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retry_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rpm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rpm_controller\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_times_executed\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retry_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rpm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rpm_controller\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_times_executed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_times_executed\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retry_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             result = self.agent_executor.invoke(\n\u001b[0m\u001b[1;32m    334\u001b[0m                 {\n\u001b[1;32m    335\u001b[0m                     \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtask_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask_for_human_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ask_for_human_input\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mformatted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask_for_human_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgentFinish\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_within_rpm_limit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_within_rpm_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     answer = self.llm.call(\n\u001b[0m\u001b[1;32m    116\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/llm.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, messages, callbacks)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m                 )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n\u001b[0;32m--> 998\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    874\u001b[0m                     \u001b[0mprint_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error while checking max token limit: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"stream\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2956\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   2959\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeployment_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"azure\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(\n\u001b[0m\u001b[1;32m    932\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\u001b[0m in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadRequestError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             error_str = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/get_llm_provider_logic.py\u001b[0m in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0merror_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model={model}\\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             raise litellm.exceptions.BadRequestError(  # type: ignore\n\u001b[0m\u001b[1;32m    329\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=google/paligemma\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
          ]
        }
      ],
      "source": [
        "support_report_crew.test(n_iterations=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d1fa2c2-6256-4420-8b7b-c8d5e09bbc01",
      "metadata": {
        "id": "3d1fa2c2-6256-4420-8b7b-c8d5e09bbc01"
      },
      "source": [
        "## Training your crew and agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4e362d-c010-43dd-88c4-e7db87834fb4",
      "metadata": {
        "height": 30,
        "id": "5a4e362d-c010-43dd-88c4-e7db87834fb4"
      },
      "outputs": [],
      "source": [
        "support_report_crew.train(n_iterations=1, filename='training.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e94a74-8700-4f83-b732-01b8d63dd774",
      "metadata": {
        "id": "39e94a74-8700-4f83-b732-01b8d63dd774"
      },
      "source": [
        "## Comparing new test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc73842-8162-49f9-b4a1-5dc7d86f3e48",
      "metadata": {
        "height": 30,
        "id": "0fc73842-8162-49f9-b4a1-5dc7d86f3e48"
      },
      "outputs": [],
      "source": [
        "support_report_crew.test(n_iterations=1, openai_model_name='gpt-4o')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d1437cf-ebc1-4e46-b982-7eeec5462479",
      "metadata": {
        "id": "1d1437cf-ebc1-4e46-b982-7eeec5462479"
      },
      "source": [
        "## Kicking off Crew"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\"image\": \"https://i0.wp.com/post.healthline.com/wp-content/uploads/2022/01/AdobeStock_272166192-1296x972.jpeg?w=1155&h=4032\"}"
      ],
      "metadata": {
        "id": "eGmMmZBmLzW1"
      },
      "id": "eGmMmZBmLzW1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a655927f-c10c-4c06-852a-d9c96fdfbfb9",
      "metadata": {
        "height": 30,
        "id": "a655927f-c10c-4c06-852a-d9c96fdfbfb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b391741-55fd-4434-a900-3e13bd3dfb29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mDermatologist\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze a skin lesion https://i0.wp.com/post.healthline.com/wp-content/uploads/2022/01/AdobeStock_272166192-1296x972.jpeg?w=1155&h=4032 using the 7-point checklist methodology for melanoma diagnosis.  This diagnostic framework evaluates specific dermatoscopic criteria categorized as major and minor.  The task involves: - Identifying major criteria: atypical pigment network, blue-white veil, and atypical vascular pattern. - Identifying minor criteria: irregular streaks, irregular pigmentation, irregular dots and globules, and regression structures. - Scoring each identified feature according to the checklist (3 points for major criteria, 1 point for minor criteria). - Summarizing the total score and providing a diagnostic interpretation based on established thresholds.\n",
            "The goal is to deliver a structured and evidence-based diagnosis of the lesion, helping clinicians assess malignancy risk accurately.\n",
            "\u001b[00m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:LiteLLM call failed: litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mDermatologist\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze a skin lesion https://i0.wp.com/post.healthline.com/wp-content/uploads/2022/01/AdobeStock_272166192-1296x972.jpeg?w=1155&h=4032 using the 7-point checklist methodology for melanoma diagnosis.  This diagnostic framework evaluates specific dermatoscopic criteria categorized as major and minor.  The task involves: - Identifying major criteria: atypical pigment network, blue-white veil, and atypical vascular pattern. - Identifying minor criteria: irregular streaks, irregular pigmentation, irregular dots and globules, and regression structures. - Scoring each identified feature according to the checklist (3 points for major criteria, 1 point for minor criteria). - Summarizing the total score and providing a diagnostic interpretation based on established thresholds.\n",
            "The goal is to deliver a structured and evidence-based diagnosis of the lesion, helping clinicians assess malignancy risk accurately.\n",
            "\u001b[00m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:LiteLLM call failed: litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mDermatologist\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze a skin lesion https://i0.wp.com/post.healthline.com/wp-content/uploads/2022/01/AdobeStock_272166192-1296x972.jpeg?w=1155&h=4032 using the 7-point checklist methodology for melanoma diagnosis.  This diagnostic framework evaluates specific dermatoscopic criteria categorized as major and minor.  The task involves: - Identifying major criteria: atypical pigment network, blue-white veil, and atypical vascular pattern. - Identifying minor criteria: irregular streaks, irregular pigmentation, irregular dots and globules, and regression structures. - Scoring each identified feature according to the checklist (3 points for major criteria, 1 point for minor criteria). - Summarizing the total score and providing a diagnostic interpretation based on established thresholds.\n",
            "The goal is to deliver a structured and evidence-based diagnosis of the lesion, helping clinicians assess malignancy risk accurately.\n",
            "\u001b[00m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:LiteLLM call failed: litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenAIError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m                         headers, response = (\n\u001b[0;32m--> 565\u001b[0;31m                             self.make_sync_openai_chat_completion_request(\n\u001b[0m\u001b[1;32m    566\u001b[0m                                 \u001b[0mopenai_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopenai_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\u001b[0m in \u001b[0;36mmake_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\u001b[0m in \u001b[0;36mmake_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             raw_response = openai_client.chat.completions.with_raw_response.create(\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         )\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1607\u001b[0m                 )\n\u001b[0;32m-> 1608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m                 response = openai_chat_completions.completion(\n\u001b[0m\u001b[1;32m   1582\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0merror_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    650\u001b[0m                 \u001b[0mstatus_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOpenAIError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1ade4cf5a00d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupport_report_crew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkickoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/crew.py\u001b[0m in \u001b[0;36mkickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sequential_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhierarchical\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_hierarchical_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/crew.py\u001b[0m in \u001b[0;36m_run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_sequential_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCrewOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;34m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_hierarchical_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCrewOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/crew.py\u001b[0m in \u001b[0;36m_execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                 task_output = task.execute_sync(\n\u001b[0m\u001b[1;32m    759\u001b[0m                     \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent_to_use\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                     \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/task.py\u001b[0m in \u001b[0;36mexecute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    190\u001b[0m     ) -> TaskOutput:\n\u001b[1;32m    191\u001b[0m         \u001b[0;34m\"\"\"Execute the task synchronously.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/task.py\u001b[0m in \u001b[0;36m_execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_by_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         result = agent.execute_task(\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_times_executed\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retry_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rpm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rpm_controller\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_times_executed\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retry_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rpm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rpm_controller\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_times_executed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_times_executed\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retry_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36mexecute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             result = self.agent_executor.invoke(\n\u001b[0m\u001b[1;32m    346\u001b[0m                 {\n\u001b[1;32m    347\u001b[0m                     \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtask_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask_for_human_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ask_for_human_input\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mformatted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask_for_human_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/crew_agent_executor.py\u001b[0m in \u001b[0;36m_invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgentFinish\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_within_rpm_limit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_within_rpm_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     answer = self.llm.call(\n\u001b[0m\u001b[1;32m    116\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/llm.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, messages, callbacks)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n\u001b[0;32m-> 1005\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0mprint_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error while checking max token limit: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"stream\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   2939\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"litellm_response_headers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm_response_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0merror_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLITELLM_EXCEPTION_TYPES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m429\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                         \u001b[0mexception_mapping_worked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                         raise RateLimitError(\n\u001b[0m\u001b[1;32m    381\u001b[0m                             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"RateLimitError: {exception_provider} - {message}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "result = support_report_crew.kickoff(inputs=input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d039f919-2d28-4871-b316-8813e9adebde",
      "metadata": {
        "id": "d039f919-2d28-4871-b316-8813e9adebde"
      },
      "source": [
        "## Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e861574-0f9b-4f2c-b2d1-a230fc3a53a3",
      "metadata": {
        "height": 47,
        "id": "9e861574-0f9b-4f2c-b2d1-a230fc3a53a3"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "display(Markdown(result.raw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a951d6a-8363-44f4-8783-e8397f435a32",
      "metadata": {
        "height": 30,
        "id": "1a951d6a-8363-44f4-8783-e8397f435a32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa40b53e-0a49-4198-a263-c79a6a3af603",
      "metadata": {
        "height": 30,
        "id": "fa40b53e-0a49-4198-a263-c79a6a3af603"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5fb6a8a-7ae7-4ae6-99a7-92aa09d97d7f",
      "metadata": {
        "height": 30,
        "id": "f5fb6a8a-7ae7-4ae6-99a7-92aa09d97d7f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66b3b165-2a4b-46cc-93f7-696dffff1e10",
      "metadata": {
        "height": 30,
        "id": "66b3b165-2a4b-46cc-93f7-696dffff1e10"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c2a2af-3e55-4de9-86bb-d33c21ade238",
      "metadata": {
        "height": 30,
        "id": "a7c2a2af-3e55-4de9-86bb-d33c21ade238"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "109926d3-50e5-43e7-87f8-a1d38e45d79d",
      "metadata": {
        "height": 30,
        "id": "109926d3-50e5-43e7-87f8-a1d38e45d79d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}